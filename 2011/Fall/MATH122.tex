\documentclass{article}
\usepackage{amsmath,amsthm,amsfonts}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}
\usepackage{fullpage}

\begin{document}
\title{Math 122 (Probability and Statistics) Study Guide}
\author{Chris Barna (chris@unbrain.net)}
\date{Fall 2011}

\maketitle

\section{Sample Space \& Events}

\subsection{Sample Space}
The sample space, $S$, is the set of all possible outcomes from an event.

\begin{enumerate}
\item \textbf{Discrete Sample Space:}
  A discrete sample space has a finite or a countably infinite number of
  elements.
\item \textbf{Continuous Sample Space:}
  A continuous sample space has an uncountably infinite number of elements.
\end{enumerate}

\subsection{Event}
An event, $A$, is a subset of the sample space.

\begin{enumerate}
\item \textbf{Simple Event:}
  A simple event, $E$, is a single element of $S$.
\end{enumerate}

\section{Probability}
The probability of an event, denoted $P(A)$, is the likelihood of the event
occuring.

\begin{enumerate}
\item $P(A) = [0, 1]$
\item $P(S) = 1$
\item If $A_1, A_2, ..., A_n$ are mutually disjoint (also mutually exclusive),
that is $A_i \bigcap A_j = \emptyset$ if $i \ne j$ then the probability of the
union is $P(\bigcup_{i=1}^{n}a_i = \sum_{i=1}{n}P(A_i))$.
\end{enumerate}

\subsection{Additive Law of Probability}
\begin{equation}
  P(A \bigcup B) = P(A) + P(B) - P(A \bigcap B)
\end{equation}

where $P(AB) = P(A \bigcap B)$.

\subsection{Multiplicative Law of Probability}
If $C \subset B \subset A$ then $P(C) = P(A) \cdot P(B|A) \cdot P(C|B)$.

\subsection{Conditional Probability}
Conditional probability of $A$ given $B$ (noted $P(A|B)$) is the probability
that $A$ will occur given that we know $B$ will occur.

\begin{equation}
  P(A|B) = \frac{P(A \bigcap B)}{P(B)}
\end{equation}

\subsection{Independence}
$A$ and $B$ are independent if the occurence (or nonoccurence) of $A$ or $B$
has no influence on the occurence of the other event.

\begin{equation}
  P(A \bigcap B) = P(A)P(B)
\end{equation}

\subsection{Total Probability}
If $B_1, B_2, ..., B_n$ form a disjoint partition of $S$ then, for any (event)
$A$: $P(A) = \sum_{i=1}^{n}P(A|B_i)P(B_i) = \sum_{i=1}^n P(A \bigcap B_i)$.

\subsubsection{Disjoint Partitions}
$B_1, B_2, ..., B_n$ form a disjoint partition of $S$ if $B_1,...,B_n$ are
mutually disjoint and $\bigcup_{i=1}^{n}B_i = S$.

\subsection{Bayes' Rule}
Let $B_1, B_2, ..., B_n$ form a disjoint partition of $S$ for any $A$ when
$P(A) \ne 0$.

\begin{equation}
  P(B_j|A) = \frac{P(A|B_j)P(B_j)}{\sum_{i=1}^{n} P(A|B_i)P(B_i)}
\end{equation}

\section{Orderings, etc...}
\subsection{Permutations}
The number of permutations is the number of ways we can take $r$ ordered
objects from $n$ objects. Denoted $_nP_r$.

\begin{equation}
_nP_r = \frac{n!}{(n-r)!}
\end{equation}

\subsection{Combinations}
The number of combinations $_nC_r$ or $n \choose r$ is the number of ways you
can take $r$ unordered objects from $n$ objects.

\begin{equation}
  {n\choose r} = \frac{n!}{r!(n-r)!}
\end{equation}

\subsubsection{Binomial Theorem}
\begin{equation}
(x+y)^n = \sum_{r=0}^{n} {n \choose r}x^n y^{n-r}
\end{equation}

\subsection{Multinomial Combinations}
The number of ways we can take $n_1, n_2, ...  n_m$ objects in $k$ groups from
$n$ objects where order within each group doesn't matter is
$\frac{n!}{n_1!n_2!...n_m!}$.

\section{Random Variables}
A random variable (RV) is a function from $S$, the sample space, to a subset
of $\mathbb{R}$, the set of all real numbers. Denoted with capital letters.

\begin{enumerate}
\item $P(Y=y)$ or $p(y)$ means the probability that we will choose a simple
  event that is mapped to the value.
\end{enumerate}

\section{Discrete Distributions}
A distribution of an RV is all the possible values taken by the RV along with
the probability of each of these values.

\subsection{Expected Value}
The average value of a random variable ($Y$). Written $E[Y]$ or $\mu_Y$.

\begin{equation}
\sum_Y y p(y) = E[Y]
\end{equation}

\subsubsection{Variance}
\begin{equation}
V[Y] = E[(y-\mu_Y)^2] = E[Y]^2 - \mu^2
\end{equation}

\begin{equation}
SD[Y] = \sigma_Y = \sqrt{V[Y]}
\end{equation}

\subsection{Geometric Distributions}
First, $\sum_{n=0}^{\infty} x^n = \frac{1}{1-x}$. If probability of "success"
is $p$, the probability of "failure" is $q=1-p$, and $Y$ is the number of
attempts until success, $p(y)=q^{y-1}p$.

\begin{enumerate}
  \item $E[Y] = \frac{1}{p}$.
  \item $V[Y] = \frac{1-p}{p^2}$.
\end{enumerate}

\subsection{Poisson Distributions}
Divide period of time into $n$ equal subintervals. Probability of an event
happening during a given $n$ is $p$. More generally, $np = \lambda$. So
$p(y)=\frac{\lambda^y}{y!}e^{-1}$. Worth knowing that
$\sum_{0}^{n}\frac{x^n}{x!}=e^x$.

\begin{enumerate}
  \item $E[Y]=\lambda$.
  \item $V[Y]=\lambda$.
\end{enumerate}

\subsection{Moment Generating Functions}
The $k$th moment of a random variable $Y$ taken about the origin is defined to
be $E[Y^k]$ and is denoted by $\mu'_k$.

The \textbf{moment generating function} $m(t)$ for a random variable $Y$ is
defined to be $m(t)=E(e^{tY})$. We say that a moment-generating function for
$Y$ exists if there exists a positive constant $b$ such that $m(t)$ is finite
for $|t| \leq b$.

If $m(t)$ exists, the $k$th deivative of $m(t)$ with respect to $t$ is
$\mu'_k$ when you set $t=0$.

\subsection{Tchebysheff's Theorem}
Let $Y$ be a random variable with mean $\mu$ and finite variance $\sigma^2$.
Then, for any constant $k>0$, $P(|Y-\mu|<k\sigma)\geq 1-\frac{1}{k^2}$.

\section{Continuous Distributions}
A continuous random variable can take any value in an interval of $\mathbb{R}$.
The distribution of $Y$ is given by $f(y)$, the probability density function
(pdf) which looks a lot like a histogram.

Can sometimes be described as $F(y)$ the (cumulative) distribution function.

\begin{enumerate}
  \item $P(a \leq Y \leq b) = \int_a^bf(y)dy$
  \item $0 \leq f(y)$
  \item $\int_{-\infty}{\infty} f(y)dy = 1$
  \item $\int_a^a f(y) dy = 0$
\end{enumerate}

\subsection{Expected Value}
\begin{equation}
  E[Y] = \int_{-\infty}^{\infty} y f(y) dy
\end{equation}
\begin{equation}
  E[y] = \int_{-\infty}^{\infty} g(y) f(y) dy
\end{equation}

\subsection{Uniform Distributions}
$Y$ is uniformly distributed between $a$ and $b$ if it has an equal chance of
taking any value between $a$ and $b$.
\begin{equation}
  f(y) =
    \begin{cases}
      \frac{1}{B-A}  & \text{if } A \leq y \leq B \\
      0  & \text{otherwise}
    \end{cases}
\end{equation}

\begin{equation}
  E[Y]=\frac{B+A}{2}
\end{equation}
\subsection{Normal Distributions}
\begin{equation}
  f(y) = \frac{1}{\sqrt{2\pi}} e^{\frac{-(y-\mu)^2}{2\sigma^2}}
\end{equation}
Normal curves are bell shaped. The top of the bell corresponds to $y=\mu$.
Points of inflection correspond to $y=\mu \pm \sigma$.

\begin{enumerate}
  \item $P(\mu - \sigma \leq Y \leq \mu + \sigma) \approx .68$
  \item $P(\mu - 2\sigma \leq Y \leq \mu + 2\sigma) \approx .95$
  \item $P(\mu - 3\sigma \leq Y \leq \mu + 3\sigma) \approx .997$
\end{enumerate}

All normal random variables' probabilities can be reduced to the probability
for just the standard normal ($z$). Use a $z$-chart where
$z=\frac{y-\mu}{\sigma}$

\subsubsection{Moment Generating Functions}
For a normally distributed RV with mean $\mu$ and variance $\sigma^2$:

\begin{equation}
  m(t) = E[e^{tY}] = e^{\mu t + \frac{\sigma^2}{2}\mu^2}
\end{equation}

\section{Multivariate Probabilities}
\subsection{Joint Probability Functions}
For a discrete probability, $P(Y_1=y_1, ..., Y_n=y_n) = p(y_1,...,y_n)$ is
called the joint probability function. For a continuous probability,
$f(y_1,y_2,...,y_n)$ is the joint probability density density function.

\subsubsection{Properties}
\begin{enumerate}
  \item $0 \leq p(y_1,y_2,..., y_n) \leq 1$ for discrete.
    $0 \leq f(y_1,y_2,...y_n)$ for continuous.
  \item $\sum_{Y_1}\sum_{Y_2}\cdots\sum_{Y_N} p(y_1, y_2, ..., y_n) = 1$ for
    discrete.
    $\int_{-\infty}^{\infty}\cdots\int_{-\infty}{\infty} f(y_1, y_2,...y_n) dy_1..dy_n$
    for continuous.
\end{enumerate}

\subsubsection{Marginal Densities and Probabilities}
Often we only care about the probability of only one of the RVs. To isolate
it's behaviour, we loot at marginal probabilities.

\[ P(Y_1=y) = p_1(y) = \sum_{y_2}\cdots\sum_{y_n} p(y, y_2, ..., y_n) \]

Similar for continuous:

\[ f_1(y) = \int_{-\infty}^{\infty}\cdots\int_{-\infty}{\infty} f(y, y_2,...y_n) dy_2..dy_n \]

\subsection{Expected Values}
\[ E[g(Y_1, Y_2, ..., Y_n)] = \sum_{Y_1}\sum_{Y_2}\cdots\sum_{Y_n}
g(y_1,...,y_n) p(y_1, ..., y_n)\]

\[ \int_{-\infty}^{\infty}\cdots\int_{-\infty}{\infty} g(y_1,...,y_n)
f(y_1,...,y_n) dy_1 \cdots dy_n \]

For binomial, $E[Y] = np$
\subsection{Covariance and Correlation}
We want to know how related the behavior of 2 RVs, $Y_1$ and $Y_2$ is.
Covariance measures this.

\[ \text{Cov}[Y_1, Y_2] = E[(Y_1 - \mu_1)(Y_2 - \mu_2)] \]

\begin{enumerate}
  \item $\text{Cov}[Y_1, Y_1] = \text{Var}[Y_1]$
  \item $\text{Cov}[Y_1, Y_2] = E[Y_1,Y_2] - \mu_1\mu_2$
\end{enumerate}

We can normalize this to make it a bit more meaningful:

\[
\rho_{Y_1,Y_2} = \text{Cor}[Y_1,Y_2] =
\frac{\text{Cov}[Y_1,Y_2]}{\sigma_1\sigma_2} = E[\frac{Y_1 -
\mu_1}{\sigma_1}\frac{Y_2 - \mu_2}{\sigma_2}]
\]
\subsection{Independent RVs}
Two discrete RVs are independent if and only if
$P(Y_1=y_1,Y_2=y_2)=p_1(y_1)p_2(y_2)$. Similarly, two continuous RVs are
independent if $f(y_1,y_2) = f_1(y_1)f_2(y_2)$.

If $f(y_1,y_2) = g(y_1)h(y_2)$ for $y_1, y_2$ in the rectangle $y_1\in[a,b]$
and $y_2\in[c,d]$ and $f(y_1,y_2) = 0$ outside this rectangle then $Y_1$ and
$Y_2$ are independent.

$E[g(y_1)h(y_2)] = E[g(Y_1)]E[h(Y_2)]$ if $Y_1$ and $Y_2$ are independent.

\subsection{Special Variance Something}
If $Y=c_1y_1 + c_2y_2 + \cdots + c_ny_n$, then:

\[ V[Y] = \sum_{i=1}^{n}c_i^2V[Y_i] +
\sum_{i=1}^{n}\sum_{j=1}^{n} c_i c_j \text{Cov}[Y_i,Y_j]\]

\section{Central Limit Theorem}
As $n$ goes to infinity of the sample mean, the distribution of $\bar{Y}$ gets
more and more normal. If $Y_i$ is nor normal, $\bar{Y}$ is usually essentailly
normal when $n \geq 25$.

\subsection{Sample Mean}
Let $Y_1,Y_2,...,Y_n$ be $n$ samples that are independent and identically
distributed. The sample mean is $\bar{Y} = \frac{\sum Y_i}{n}$. The mean of
$\bar{Y}$ is $\mu$. The standard devition of $\bar{Y}$ is
$\frac{\sigma}{\sqrt{n}}$

\subsection{Applications}
\subsubsection{Weak Law of Large Numbers}
For every $\epsilon > 0$,
$\lim_{n \to \infty} P(|\bar{Y}_n - \mu| > \epsilon) = 0$.
\subsubsection{Strong Law of Large Numbers}
$\lim_{n \to \infty} \bar{Y}_n = \mu$ for $s \in S - \mathbb{S}$ where
$P(\mathbb{S}) = 0$.

\section{Functions of Random Variables}
\subsection{Method of Transformations}
Often we know $f_Y(y)$, the pdf for a RV $Y$ but we then want to know $f_U(u)$
where $U=h(Y)$. That is, we want the pdf of a function of y.

We know $P(y_1 \leq Y \leq y_2) = P(h(y_1) \leq h(Y) \leq h(y_2)$ and that
$\int_{y_1}^{y_2}f_Y(y)dy=\int_{h(y_1)}^{h(y_2)}f_U(u)du$. From this, we can
establish the folowing formula:

\[f_U(u) = f_Y(h^{-1}(u)) \frac{dh^{-1}(u)}{du} = f_Y(y)\frac{dy}{du}\]
\subsection{Computer Generation of Standard Normal Numbers}

$z=\sqrt{-2\text{ln}y_1}\cos(2\pi y_2)$ where $Y_1$ and $Y_2$ are two sucessive
numbers from a computer generated uniform number between 0 and 1.
\subsection{Order Statistics}
\[f_{Y_{max}} = P(Y_{max} \leq y) = n(F(y))^{n-1}f(y)\]
\[f_{Y_{min}} = P(Y_{min} > y) = n[1-F(y)]^{n-1}f(y)\]
\end{document}
