\documentclass{article}
\usepackage{amsmath,amsthm,amsfonts}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}
% \usepackage{fullpage}

\begin{document}
\title{Math 122 (Probability and Statistics) Study Guide}
\author{Chris Barna (chris@unbrain.net)}
\date{Fall 2011}

\maketitle

\section{Sample Space \& Events}

\subsection{Sample Space}
The sample space, $S$, is the set of all possible outcomes from an event.

\begin{enumerate}
\item \textbf{Discrete Sample Space:}
  A discrete sample space has a finite or a countably infinite number of
  elements.
\item \textbf{Continuous Sample Space:}
  A continuous sample space has an uncountably infinite number of elements.
\end{enumerate}

\subsection{Event}
An event, $A$, is a subset of the sample space.

\begin{enumerate}
\item \textbf{Simple Event:}
  A simple event, $E$, is a single element of $S$.
\end{enumerate}

\section{Probability}
The probability of an event, denoted $P(A)$, is the likelihood of the event
occuring.

\begin{enumerate}
\item $P(A) = [0, 1]$
\item $P(S) = 1$
\item If $A_1, A_2, ..., A_n$ are mutually disjoint (also mutually exclusive),
that is $A_i \bigcap A_j = \emptyset$ if $i \ne j$ then the probability of the
union is $P(\bigcup_{i=1}^{n}a_i = \sum_{i=1}{n}P(A_i))$.
\end{enumerate}

\subsection{Additive Law of Probability}
\begin{equation}
  P(A \bigcup B) = P(A) + P(B) - P(A \bigcap B)
\end{equation}

where $P(AB) = P(A \bigcap B)$.

\subsection{Multiplicative Law of Probability}
If $C \subset B \subset A$ then $P(C) = P(A) \cdot P(B|A) \cdot P(C|B)$.

\subsection{Conditional Probability}
Conditional probability of $A$ given $B$ (noted $P(A|B)$) is the probability
that $A$ will occur given that we know $B$ will occur.

\begin{equation}
  P(A|B) = \frac{P(A \bigcap B)}{P(B)}
\end{equation}

\subsection{Independence}
$A$ and $B$ are independent if the occurence (or nonoccurence) of $A$ or $B$
has no influence on the occurence of the other event.

\begin{equation}
  P(A \bigcap B) = P(A)P(B)
\end{equation}

\subsection{Total Probability}
If $B_1, B_2, ..., B_n$ form a disjoint partition of $S$ then, for any (event)
$A$: $P(A) = \sum_{i=1}^{n}P(A|B_i)P(B_i) = \sum_{i=1}^n P(A \bigcap B_i)$.

\subsubsection{Disjoint Partitions}
$B_1, B_2, ..., B_n$ form a disjoint partition of $S$ if $B_1,...,B_n$ are
mutually disjoint and $\bigcup_{i=1}^{n}B_i = S$.

\subsection{Bayes' Rule}
Let $B_1, B_2, ..., B_n$ form a disjoint partition of $S$ for any $A$ when
$P(A) \ne 0$.

\begin{equation}
  P(B_j|A) = \frac{P(A|B_j)P(B_j)}{\sum_{i=1}^{n} P(A|B_i)P(B_i)}
\end{equation}

\section{Orderings, etc...}
\subsection{Permutations}
The number of permutations is the number of ways we can take $r$ ordered
objects from $n$ objects. Denoted $_nP_r$.

\begin{equation}
_nP_r = \frac{n!}{(n-r)!}
\end{equation}

\subsection{Combinations}
The number of combinations $_nC_r$ or $n \choose r$ is the number of ways you
can take $r$ unordered objects from $n$ objects.

\begin{equation}
  {n\choose r} = \frac{n!}{r!(n-r)!}
\end{equation}

\subsubsection{Binomial Theorem}
\begin{equation}
(x+y)^n = \sum_{r=0}^{n} {n \choose r}x^n y^{n-r}
\end{equation}

\subsection{Multinomial Combinations}
The number of ways we can take $n_1, n_2, ...  n_m$ objects in $k$ groups from
$n$ objects where order within each group doesn't matter is
$\frac{n!}{n_1!n_2!...n_m!}$.

\section{Random Variables}
A random variable (RV) is a function from $S$, the sample space, to a subset
of $\mathbb{R}$, the set of all real numbers. Denoted with capital letters.

\begin{enumerate}
\item $P(Y=y)$ or $p(y)$ means the probability that we will choose a simple
  event that is mapped to the value.
\end{enumerate}

\section{Discrete Distributions}
A distribution of an RV is all the possible values taken by the RV along with
the probability of each of these values.

\subsection{Expected Value}
The average value of a random variable ($Y$). Written $E[Y]$ or $\mu_Y$.

\begin{equation}
\sum_Y y p(y) = E[Y]
\end{equation}

\subsubsection{Variance}
\begin{equation}
V[Y] = E[(y-\mu_Y)^2] = E[Y]^2 - \mu^2
\end{equation}

\begin{equation}
SD[Y] = \sigma_Y = \sqrt{V[Y]}
\end{equation}

\subsection{Geometric Distributions}
First, $\sum_{n=0}^{\infty} x^n = \frac{1}{1-x}$. If probability of "success"
is $p$, the probability of "failure" is $q=1-p$, and $Y$ is the number of
attempts until success, $p(y)=q^{y-1}p$.

\begin{enumerate}
  \item $E[Y] = \frac{1}{p}$.
  \item $V[Y] = \frac{1-p}{p^2}$.
\end{enumerate}

\subsection{Poisson Distributions}
Divide period of time into $n$ equal subintervals. Probability of an event
happening during a given $n$ is $p$. More generally, $np = \lambda$. So
$p(y)=\frac{\lambda^y}{y!}e^{-1}$. Worth knowing that
$\sum_{0}^{n}\frac{x^n}{x!}=e^x$.

\begin{enumerate}
  \item $E[Y]=\lambda$.
  \item $V[Y]=\lambda$.
\end{enumerate}

\subsection{Moment Generating Functions}
The \textbf{moment generating function} $m(t)$ for a random variable $Y$ is
defined to be $m(t)=E(e^{tY})$. We say that a moment-generating function for
$Y$ exists if there exists a positive constant $b$ such that $m(t)$ is finite
for $|t| \leq b$.

\subsection{Tchebysheff's Theorem}
Let $Y$ be a random variable with mean $\mu$ and finite variance $\sigma^2$.
Then, for any constant $k>0$, $P(|Y-\mu|<k\sigma)\geq 1-\frac{1}{k^2}$.

\section{Continuous Distributions}
A continuous random variable can take any value in an interval of $\mathbb{R}$.
The distribution of $Y$ is given by $f(y)$, the probability density function
(pdf) which looks a lot like a histogram.

Can sometimes be described as $F(y)$ the (cumulative) distribution function.

\begin{enumerate}
  \item $P(a \leq Y \leq b) = \int_a^bf(y)dy$
  \item $0 \leq f(y)$
  \item $\int_{-\infty}{\infty} f(y)dy = 1$
  \item $\int_a^a f(y) dy = 0$
\end{enumerate}

\subsection{Expected Value}
\begin{equation}
  E[Y] = \int_{-\infty}^{\infty} y f(y) dy
\end{equation}
\begin{equation}
  E[y] = \int_{-\infty}^{\infty} g(y) f(y) dy
\end{equation}
\end{document}
